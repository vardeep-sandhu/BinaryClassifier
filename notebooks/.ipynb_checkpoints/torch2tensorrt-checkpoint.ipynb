{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc05744",
   "metadata": {},
   "source": [
    "This is the torch to tensorrt converter notebook. I will add more stuff once I know what to add. I am following this link: https://learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ceeaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967f737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = \"../runs/traversky/2023_03_05_21_30_33/model.pth\"\n",
    "ONNX_FILE_PATH = 'bin_classifier.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d7d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(2048, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d932368c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model loaded\n",
    "checkpoint = torch.load(trained_model_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de668a4b",
   "metadata": {},
   "source": [
    "Now the tensorrt conversion. \n",
    "\n",
    "First step is to make .onnx file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcf7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.empty((1, 3, 256, 256))\n",
    "torch.onnx.export(model, dummy_input, ONNX_FILE_PATH, input_names=['input'],\n",
    "                  output_names=['output'], export_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3221cf",
   "metadata": {},
   "source": [
    "Now lets see the tensor rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1f5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef9c5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logger\n",
    "\n",
    "def build_engine(onnx_file_path):\n",
    "    # initialize TensorRT engine and parse ONNX model\n",
    "    TRT_LOGGER = trt.Logger()\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    profile = builder.create_optimization_profile()\n",
    "    config = builder.create_builder_config()\n",
    "# Setting engine_precesion to FP16\n",
    "    config.set_flag(trt.BuilderFlag.FP16)\n",
    "    print(config)\n",
    "#     parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "    \n",
    "#     # parse ONNX\n",
    "#     with open(onnx_file_path, 'rb') as model:\n",
    "#         print('Beginning ONNX file parsing')\n",
    "#         parser.parse(model.read())\n",
    "#     print('Completed parsing of ONNX file')\n",
    "#     print(network.get_output(0))\n",
    "#     # allow TensorRT to use up to 1GB of GPU memory for tactic selection\n",
    "# #     builder.max_workspace_size = 1 << 30\n",
    "#     # we have only one image in batch\n",
    "# #     builder.max_batch_size = 1\n",
    "#     # use FP16 mode if possible\n",
    "# #     if builder.platform_has_fast_fp16:\n",
    "# #         builder.fp16_mode = True\n",
    "#     # generate TensorRT engine optimized for the target platform\n",
    "#     print('Building an engine...')\n",
    "#     engine = builder.build_serialized_network(network, builder_config)\n",
    "#     context = engine.create_execution_context()\n",
    "#     print(\"Completed creating Engine\")\n",
    " \n",
    "#     return engine, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "039a03f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/26/2023-11:46:00] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'engine_precision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbuild_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbin_classifier.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m, in \u001b[0;36mbuild_engine\u001b[0;34m(onnx_file_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m profile \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mcreate_optimization_profile()\n\u001b[1;32m      9\u001b[0m config \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mcreate_builder_config()\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mengine_precision\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP16\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     11\u001b[0m     config\u001b[38;5;241m.\u001b[39mset_flag(trt\u001b[38;5;241m.\u001b[39mBuilderFlag\u001b[38;5;241m.\u001b[39mFP16)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'engine_precision' is not defined"
     ]
    }
   ],
   "source": [
    "build_engine('bin_classifier.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c083fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def build_engine(batch_sizes, workspace_size, sequence_lengths, config, weights_dict, squad_json, vocab_file, calibrationCacheFile, calib_num):\n",
    "#     explicit_batch_flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "#     with trt.Builder(TRT_LOGGER) as builder, builder.create_network(explicit_batch_flag) as network, builder.create_builder_config() as builder_config:\n",
    "#         builder_config.max_workspace_size = workspace_size * (1024 * 1024)\n",
    "#         if config.use_fp16:\n",
    "#             builder_config.set_flag(trt.BuilderFlag.FP16)\n",
    "#         if config.use_int8:\n",
    "#             builder_config.set_flag(trt.BuilderFlag.INT8)\n",
    "#             if not config.use_qat:\n",
    "#                 calibrator = BertCalibrator(squad_json, vocab_file, calibrationCacheFile, 1, sequence_lengths[-1], calib_num)\n",
    "#                 builder_config.set_quantization_flag(trt.QuantizationFlag.CALIBRATE_BEFORE_FUSION)\n",
    "#                 builder_config.int8_calibrator = calibrator\n",
    "#         if config.use_strict:\n",
    "#             builder_config.set_flag(trt.BuilderFlag.STRICT_TYPES)\n",
    "    \n",
    "#         if config.use_sparsity:\n",
    "#             TRT_LOGGER.log(TRT_LOGGER.INFO, \"Setting sparsity flag on builder_config.\")\n",
    "#             builder_config.set_flag(trt.BuilderFlag.SPARSE_WEIGHTS)\n",
    "\n",
    "#         # speed up the engine build for trt major version >= 8 \n",
    "#         # 1. disable cudnn tactic\n",
    "#         # 2. load global timing cache\n",
    "#         if trt_version[0] >= 8:\n",
    "#             tactic_source = 1 << int(trt.TacticSource.CUBLAS) | 1 << int(trt.TacticSource.CUBLAS_LT)\n",
    "#             builder_config.set_tactic_sources(tactic_source)\n",
    "#             if config.timing_cache != None:\n",
    "#                 if os.path.exists(config.timing_cache):\n",
    "#                     with open(config.timing_cache, \"rb\") as f:\n",
    "#                         cache = builder_config.create_timing_cache(f.read())\n",
    "#                         builder_config.set_timing_cache(cache, ignore_mismatch = False)\n",
    "#                 else:\n",
    "#                     cache = builder_config.create_timing_cache(b\"\")\n",
    "#                     builder_config.set_timing_cache(cache, ignore_mismatch = False)\n",
    "\n",
    "#         # only use the largest sequence when in calibration mode\n",
    "#         if config.is_calib_mode:\n",
    "#             sequence_lengths = sequence_lengths[-1:]\n",
    "\n",
    "#         # Create the network\n",
    "#         emb_layer = emb_layernorm(builder, network, config, weights_dict, builder_config, sequence_lengths, batch_sizes)\n",
    "#         embeddings = emb_layer.get_output(0)\n",
    "#         mask_idx = emb_layer.get_output(1)\n",
    "\n",
    "#         bert_out = bert_model(config, weights_dict, network, embeddings, mask_idx)\n",
    "\n",
    "#         squad_logits = squad_output(\"cls_\", config, weights_dict, network, bert_out)\n",
    "#         squad_logits_out = squad_logits.get_output(0)\n",
    "\n",
    "#         network.mark_output(squad_logits_out)\n",
    "\n",
    "#         build_start_time = time.time()\n",
    "#         engine = builder.build_engine(network, builder_config)\n",
    "#         build_time_elapsed = (time.time() - build_start_time)\n",
    "#         TRT_LOGGER.log(TRT_LOGGER.INFO, \"build engine in {:.3f} Sec\".format(build_time_elapsed))\n",
    "\n",
    "#         # save global timing cache\n",
    "#         if trt_version[0] >= 8 and config.timing_cache != None:\n",
    "#             cache = builder_config.get_timing_cache()\n",
    "#             with cache.serialize() as buffer:\n",
    "#                 with open(config.timing_cache, \"wb\") as f:\n",
    "#                     f.write(buffer)\n",
    "#                     f.flush()\n",
    "#                     os.fsync(f)\n",
    "\n",
    "#         if config.use_int8 and not config.use_qat:\n",
    "#             calibrator.free()\n",
    "#         return engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
